# Přednáška IV.
* rozhodovací stromy
* TD-IDT
    * vezmi jeden atribut jako kořen dílčího stromu
    * rozděl data na podmnožiny podle hodnot tohoto atributu
    * nepatří-li všechna data v podmnožině do téže třídy, pro tuto podmnožinu opakuj postup od bodu 1
    * omezení - kategoriální atributy, jen data bez šumu
    * volba atributu
        * entropie
        * Gini index
        * chí kvadrát
        * informační zisk
* prožezávání stormů
    * bezchybná klasifikace nezaručuje kvalitní klasifikaci dat testovacích (overfitting)
    * úplný strom může být příliš velký
    * řešení - redukce stromu, aby v listovém uzlu převažovaly příklady jedné třídy
        * pre-pruning - modifikuje se zastavovací kritérium - nebude se větvit pokud počet příkladů v uzlu klesne pod danou hodnotu nebo pokud relativní počet příkladů jedné třídy překročí danou hodnotu
        * post-pruning - vytvoří se úplný strom, který se následně redukuje
            * strom se převede na pravidla, pravidla se generalizují (pokud dojde ke zlepšení odhadované přesnosti)
            * pravidla se uspoádají podle odhadované přesnosti _ v tomto budou pravidla použita pro klasifikaci
* práce s numerickými atributy
    * algoritmus používá pouze kategorické atributy
    * off-line - v rámci přípravy a předzpracování dat
    * 
